#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CNNæ°´ä½“åˆ†ç±»æ¨¡å‹è®­ç»ƒ - ä½¿ç”¨AlphaEarthç‰¹å¾
åŸºäºgeeCNN_Water_Classification.ipynbï¼Œé€‚é…3x3çª—å£å’Œæ–°æ•°æ®æ ¼å¼
æ ¹æ®é¡¹ç›®è§„åˆ™ï¼Œä½¿ç”¨ç®€å•ç›´æ¥çš„ä»£ç 
"""

import torch
import random
import numpy as np
import pandas as pd
import ast
import matplotlib.pyplot as plt
from torch.utils.data import TensorDataset, DataLoader
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

# æ£€æŸ¥CUDAå¯ç”¨æ€§å¹¶è®¾ç½®è®¾å¤‡
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")
if torch.cuda.is_available():
    print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# è®¾ç½®é»˜è®¤æ•°æ®ç±»å‹
torch.set_default_dtype(torch.float32)

# è®¾ç½®matplotlibå­—ä½“ä¸ºArial
plt.rcParams['font.family'] = 'Arial'

print("å¼€å§‹è®­ç»ƒCNNæ°´ä½“åˆ†ç±»æ¨¡å‹...")
print("ä½¿ç”¨AlphaEarthç‰¹å¾å’Œ5x5çª—å£")

# è¯»å–æ•°æ®
print("\n1. è¯»å–æ•°æ®...")
data = pd.read_csv("data/water_CNN_with_AlphaEarth_all_merged.csv")
print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
print(f"æ•°æ®åˆ—: {list(data.columns)}")

# æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
landcover_counts = data['landcover'].value_counts()
print(f"\nåŸå§‹æ ‡ç­¾åˆ†å¸ƒ:")
for label, count in landcover_counts.items():
    percentage = (count / len(data)) * 100
    print(f"  ç±»åˆ« {label}: {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)")

# å°†æ ‡ç­¾é‡æ–°æ˜ å°„ï¼š0->0 (éæ°´ä½“), 11->1 (æ°´ä½“)
print("\né‡æ–°æ˜ å°„æ ‡ç­¾: 0->0 (éæ°´ä½“), 11->1 (æ°´ä½“)")
data['landcover_binary'] = data['landcover'].map({0: 0, 11: 1})

# æ£€æŸ¥é‡æ–°æ˜ å°„åçš„æ ‡ç­¾åˆ†å¸ƒ
binary_counts = data['landcover_binary'].value_counts()
print(f"\né‡æ–°æ˜ å°„åæ ‡ç­¾åˆ†å¸ƒ:")
for label, count in binary_counts.items():
    label_name = "éæ°´ä½“" if label == 0 else "æ°´ä½“"
    percentage = (count / len(data)) * 100
    print(f"  ç±»åˆ« {label} ({label_name}): {count} ä¸ªæ ·æœ¬ ({percentage:.1f}%)")

# æ•°æ®åˆ’åˆ†ï¼š70%è®­ç»ƒï¼Œ30%éªŒè¯
print("\n2. æ•°æ®åˆ’åˆ†...")
train_data = data[data['random'] <= 0.7]
val_data = data[data['random'] > 0.7]
print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)}")
print(f"éªŒè¯é›†å¤§å°: {len(val_data)}")

# ç‰¹å¾åˆ—ï¼šåŒ…å«AlphaEarthå’ŒSentinel-2æ³¢æ®µ
feature_columns = ['A31', 'A36', 'A46', 'A47', 'A63', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']
print(f"\nä½¿ç”¨ç‰¹å¾: {feature_columns}")
print(f"ç‰¹å¾æ•°é‡: {len(feature_columns)}")

# å¤„ç†è®­ç»ƒæ•°æ®
print("\n3. å¤„ç†è®­ç»ƒæ•°æ®...")
train_features_list = []
for col in feature_columns:
    col_data = np.array(train_data[col].apply(ast.literal_eval).values.tolist())
    train_features_list.append(col_data)

# å †å ç‰¹å¾ (æ ·æœ¬æ•°, ç‰¹å¾æ•°, 3, 3) - æ³¨æ„è¿™é‡Œæ”¹ä¸º3x3
train_features = np.stack(train_features_list, axis=1)
print(f"è®­ç»ƒç‰¹å¾å½¢çŠ¶: {train_features.shape}")

# æ•°æ®æ˜¯5x5çª—å£ï¼Œä¿æŒåŸå§‹å°ºå¯¸
print(f"ä½¿ç”¨5x5çª—å£ï¼Œè®­ç»ƒç‰¹å¾å½¢çŠ¶: {train_features.shape}")

train_features = torch.tensor(train_features, dtype=torch.float32)
train_labels = torch.tensor(train_data['landcover_binary'].values, dtype=torch.long)

# å¤„ç†éªŒè¯æ•°æ®
print("\n4. å¤„ç†éªŒè¯æ•°æ®...")
val_features_list = []
for col in feature_columns:
    col_data = np.array(val_data[col].apply(ast.literal_eval).values.tolist())
    val_features_list.append(col_data)

val_features = np.stack(val_features_list, axis=1)
print(f"éªŒè¯ç‰¹å¾å½¢çŠ¶: {val_features.shape}")

# éªŒè¯æ•°æ®ä¹Ÿæ˜¯5x5çª—å£
print(f"éªŒè¯ç‰¹å¾å½¢çŠ¶: {val_features.shape}")

val_features = torch.tensor(val_features, dtype=torch.float32)
val_labels = torch.tensor(val_data['landcover_binary'].values, dtype=torch.long)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
print("\n5. åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
train_dataset = TensorDataset(train_features, train_labels)
val_dataset = TensorDataset(val_features, val_labels)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
print(f"éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")

# å®šä¹‰CNNæ¨¡å‹ - é€‚é…5x5è¾“å…¥å’Œ15ä¸ªç‰¹å¾é€šé“ï¼Œå‚è€ƒåŸå§‹geeCNNæ¶æ„
print("\n6. å®šä¹‰CNNæ¨¡å‹...")
class WaterCNN(nn.Module):
    def __init__(self, num_features=15, num_classes=2):
        super(WaterCNN, self).__init__()

        # å‚è€ƒåŸå§‹geeCNNæ¶æ„ï¼Œé€‚é…5x5çª—å£
        self.conv1 = nn.Conv2d(in_channels=num_features, out_channels=16, kernel_size=(3, 3), padding=0)
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=0)
        self.conv3 = nn.Conv2d(in_channels=32 + num_features, out_channels=64, kernel_size=(1, 1))  # è¿æ¥ä¸­å¿ƒ1x1
        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(1, 1))
        self.conv5 = nn.Conv2d(in_channels=128, out_channels=num_classes, kernel_size=(1, 1))

    def forward(self, x):
        # x shape: (batch, 15, 5, 5)
        x1 = F.relu(self.conv1(x))  # (batch, 16, 3, 3) - 5x5 -> 3x3
        x2 = F.relu(self.conv2(x1))  # (batch, 32, 1, 1) - 3x3 -> 1x1

        # å–åŸå§‹è¾“å…¥çš„ä¸­å¿ƒ1x1åŒºåŸŸ
        center_1x1 = x[:, :, 2:3, 2:3]  # (batch, 15, 1, 1) - ä»5x5ä¸­å–ä¸­å¿ƒ

        # è¿æ¥ç‰¹å¾
        x3_input = torch.cat((x2, center_1x1), dim=1)  # (batch, 32+15=47, 1, 1)
        x3 = F.relu(self.conv3(x3_input))  # (batch, 64, 1, 1)
        x4 = F.relu(self.conv4(x3))  # (batch, 128, 1, 1)
        x5 = self.conv5(x4)  # (batch, 2, 1, 1)

        # å±•å¹³ä¸ºåˆ†ç±»è¾“å‡º
        return x5.view(x5.size(0), -1)  # (batch, 2)

# åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°GPU
model = WaterCNN(num_features=len(feature_columns), num_classes=2)
print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")

# ç¡®ä¿æ¨¡å‹å®Œå…¨ç§»åŠ¨åˆ°è®¾å¤‡
model = model.to(device)
print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")

# æ£€æŸ¥æ¨¡å‹å‚æ•°è®¾å¤‡
for name, param in model.named_parameters():
    if param.device != device:
        print(f"è­¦å‘Š: å‚æ•° {name} åœ¨è®¾å¤‡ {param.device}ï¼Œåº”è¯¥åœ¨ {device}")

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

print(f"æŸå¤±å‡½æ•°: {criterion}")
print(f"ä¼˜åŒ–å™¨: {optimizer}")

# è®­ç»ƒæ¨¡å‹
print("\n7. å¼€å§‹è®­ç»ƒ...")
num_epochs = 50
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []

best_val_acc = 0.0
best_model_state = None

for epoch in range(num_epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_loss = 0.0
    train_correct = 0
    train_total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(output.data, 1)
        train_total += target.size(0)
        train_correct += (predicted == target).sum().item()
    
    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    
    with torch.no_grad():
        for data, target in val_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)

            val_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            val_total += target.size(0)
            val_correct += (predicted == target).sum().item()
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)
    train_acc = 100 * train_correct / train_total
    val_acc = 100 * val_correct / val_total
    
    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_model_state = model.state_dict().copy()
    
    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()
    
    # æ‰“å°è¿›åº¦
    if (epoch + 1) % 5 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}]')
        print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.6f}')

print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")

# åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°
model.load_state_dict(best_model_state)

# æœ€ç»ˆè¯„ä¼°
print("\n8. æœ€ç»ˆè¯„ä¼°...")
model.eval()
all_predictions = []
all_targets = []

with torch.no_grad():
    for data, target in val_loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        _, predicted = torch.max(output.data, 1)
        all_predictions.extend(predicted.cpu().numpy())
        all_targets.extend(target.cpu().numpy())

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
final_accuracy = accuracy_score(all_targets, all_predictions)
print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.4f}")

print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(all_targets, all_predictions, 
                          target_names=['éæ°´ä½“', 'æ°´ä½“']))

# ä¿å­˜æ¨¡å‹
model_path = "model/water_cnn_alpha_model.pth"
import os
os.makedirs("model", exist_ok=True)
torch.save(best_model_state, model_path)
print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
print("\n9. ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# æŸå¤±æ›²çº¿
ax1.plot(train_losses, label='Training Loss', color='blue')
ax1.plot(val_losses, label='Validation Loss', color='red')
ax1.set_title('Training and Validation Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend()
ax1.grid(True)

# å‡†ç¡®ç‡æ›²çº¿
ax2.plot(train_accuracies, label='Training Accuracy', color='blue')
ax2.plot(val_accuracies, label='Validation Accuracy', color='red')
ax2.set_title('Training and Validation Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy (%)')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.savefig('model/training_curves.png', dpi=300, bbox_inches='tight')
plt.show()

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ
print("\n10. ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
cm = confusion_matrix(all_targets, all_predictions)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=['Non-Water', 'Water'],
            yticklabels=['Non-Water', 'Water'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.savefig('model/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nğŸ‰ CNNæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
print(f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_accuracy:.4f}")
print(f"æ¨¡å‹æ–‡ä»¶: {model_path}")
print("è®­ç»ƒæ›²çº¿: model/training_curves.png")
print("æ··æ·†çŸ©é˜µ: model/confusion_matrix.png")
